"""
Scraping script for Disney menu items
Usage:
    python scripts/scrape_menus.py --start 0 --end 100  # Test with first 100 IDs
    python scripts/scrape_menus.py                       # Scrape all (0-9999)
"""

import asyncio
import aiohttp
import json
import sys
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
import argparse

# Add parent directory to path to import api modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from api.scraper import MenuScraper


async def scrape_menu(
    session: aiohttp.ClientSession, menu_id: str, scraper: MenuScraper, semaphore: asyncio.Semaphore
) -> Optional[Dict]:
    """
    å˜ä¸€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

    Args:
        session: aiohttp session
        menu_id: ãƒ¡ãƒ‹ãƒ¥ãƒ¼IDï¼ˆ4æ¡ï¼‰
        scraper: MenuScraperã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        semaphore: åŒæ™‚æ¥ç¶šæ•°åˆ¶é™ç”¨

    Returns:
        ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã€ã¾ãŸã¯ None
    """
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: URLã‚’æ¤œè¨¼ï¼ˆSSRFå¯¾ç­–ï¼‰
    allowed_domain = "www.tokyodisneyresort.jp"
    url = f"https://{allowed_domain}/food/{menu_id}/"

    async with semaphore:
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆDoSå¯¾ç­–ï¼‰
            timeout = aiohttp.ClientTimeout(total=10, connect=5)
            async with session.get(url, timeout=timeout, allow_redirects=False) as response:
                # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’è¨±å¯ã—ãªã„ï¼ˆSSRFå¯¾ç­–ï¼‰
                if response.status == 200:
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ5MBï¼‰
                    content_length = response.headers.get("Content-Length")
                    if content_length and int(content_length) > 5 * 1024 * 1024:
                        return None

                    html = await response.text()

                    # HTMLã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                    if len(html) > 5 * 1024 * 1024:
                        return None

                    return scraper.parse_menu_page(html, menu_id)
                elif response.status == 404:
                    return None  # å­˜åœ¨ã—ãªã„ID
                else:
                    return None
        except asyncio.TimeoutError:
            return None
        except aiohttp.ClientError:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆè©³ç´°ã‚’éš ã™ï¼‰
            return None
        except Exception:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ï¼ˆè©³ç´°ã‚’éš ã™ï¼‰
            return None


async def scrape_all_menus(
    start_id: int = 0, end_id: int = 9999, rate_limit: float = 1.0, max_concurrent: int = 5
) -> List[Dict]:
    """
    å…¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

    Args:
        start_id: é–‹å§‹ID
        end_id: çµ‚äº†ID
        rate_limit: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        max_concurrent: æœ€å¤§åŒæ™‚æ¥ç¶šæ•°

    Returns:
        ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    scraper = MenuScraper()
    results = []
    semaphore = asyncio.Semaphore(max_concurrent)

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šï¼ˆã‚ˆã‚Šè©³ç´°ãªãƒ–ãƒ©ã‚¦ã‚¶ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    # TCPConnectorã§SSLæ¤œè¨¼ã‚’æœ‰åŠ¹åŒ–ã—ã€æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’è¨­å®š
    connector = aiohttp.TCPConnector(limit=max_concurrent, limit_per_host=max_concurrent, ttl_dns_cache=300, ssl=True)

    async with aiohttp.ClientSession(headers=headers, connector=connector) as session:
        total_ids = end_id - start_id + 1
        batch_size = 10  # 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†

        print(f"\nScraping menu IDs {start_id:04d} to {end_id:04d} ({total_ids} IDs)")
        print(f"Concurrent requests: {max_concurrent}")
        print(f"Batch size: {batch_size}")
        print(f"Rate limit: {rate_limit}s between batches")
        print(f"Estimated time: ~{total_ids * rate_limit / batch_size / 60:.1f} minutes\n")

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§ãƒãƒƒãƒå‡¦ç†
        with tqdm(total=total_ids, desc="Scraping", unit="menu") as pbar:
            for batch_start in range(start_id, end_id + 1, batch_size):
                batch_end = min(batch_start + batch_size - 1, end_id)

                # ãƒãƒƒãƒå†…ã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                tasks = []
                for menu_id_num in range(batch_start, batch_end + 1):
                    menu_id_str = str(menu_id_num).zfill(4)
                    tasks.append(scrape_menu(session, menu_id_str, scraper, semaphore))

                # ãƒãƒƒãƒã‚’ä¸¦è¡Œå®Ÿè¡Œ
                batch_results = await asyncio.gather(*tasks)

                # çµæœã‚’é›†ç´„
                for data in batch_results:
                    if data:
                        results.append(data)

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
                pbar.update(len(tasks))
                pbar.set_postfix({"found": len(results)})

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆæœ€å¾Œã®ãƒãƒƒãƒä»¥å¤–ï¼‰
                if rate_limit > 0 and batch_end < end_id:
                    await asyncio.sleep(rate_limit)

    return results


def save_menus(menus: List[Dict], output_path: str = "data/menus.json"):
    """
    ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’JSONã«ä¿å­˜ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸ï¼‰

    Args:
        menus: æ–°è¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        output_path: å‡ºåŠ›å…ˆãƒ‘ã‚¹
    """
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    existing_menus = {}
    if output_file.exists():
        try:
            with open(output_file, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
                existing_menus = {menu["id"]: menu for menu in existing_data}
                print(f"\nğŸ“‚ Loaded {len(existing_menus)} existing menus")
        except Exception as e:
            print(f"\nâš ï¸  Failed to load existing data: {e}")

    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ¼ã‚¸ï¼ˆæ—¢å­˜ã®categoryã‚„tagsã‚’ä¿æŒï¼‰
    merged_count = 0
    new_count = 0
    for menu in menus:
        menu_id = menu["id"]
        if menu_id in existing_menus:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®categoryã¨custom fieldsã‚’ä¿æŒ
            old_menu = existing_menus[menu_id]
            if "category" in old_menu:
                menu["category"] = old_menu["category"]
            merged_count += 1
        else:
            new_count += 1
        existing_menus[menu_id] = menu

    print(f"ğŸ“Š Merge stats: {new_count} new, {merged_count} updated")

    # ã‚½ãƒ¼ãƒˆã—ã¦ä¿å­˜
    all_menus = sorted(existing_menus.values(), key=lambda x: int(x["id"]))

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_menus, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ Saved {len(menus)} menus to {output_path}")

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    if menus:
        print("\nStatistics:")
        print(f"  Total menus: {len(menus)}")

        parks = {}
        for menu in menus:
            for restaurant in menu.get("restaurants", []):
                park = restaurant.get("park", "unknown")
                parks[park] = parks.get(park, 0) + 1

        for park, count in sorted(parks.items()):
            print(f"  {park.upper()}: {count} restaurant entries")

        # ä¾¡æ ¼çµ±è¨ˆ
        prices = [m["price"]["amount"] for m in menus if m.get("price", {}).get("amount", 0) > 0]
        if prices:
            print(f"  Price range: Â¥{min(prices)} - Â¥{max(prices)}")
            print(f"  Average price: Â¥{sum(prices) // len(prices)}")


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(description="Scrape Disney menu data")
    parser.add_argument("--start", type=int, default=0, help="Start menu ID (default: 0)")
    parser.add_argument("--end", type=int, default=9999, help="End menu ID (default: 9999)")
    parser.add_argument("--output", type=str, default="data/menus.json", help="Output file path")
    parser.add_argument(
        "--rate-limit", type=float, default=1.0, help="Rate limit between batches in seconds (default: 1.0)"
    )
    parser.add_argument("--max-concurrent", type=int, default=10, help="Max concurrent requests (default: 10)")

    args = parser.parse_args()

    print("=" * 60)
    print("Disney Menu Scraper")
    print("=" * 60)
    print(f"Range: {args.start:04d} - {args.end:04d}")
    print(f"Rate limit: {args.rate_limit} seconds")
    print(f"Max concurrent: {args.max_concurrent}")
    print(f"Output: {args.output}")
    print("=" * 60)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    menus = asyncio.run(
        scrape_all_menus(
            start_id=args.start, end_id=args.end, rate_limit=args.rate_limit, max_concurrent=args.max_concurrent
        )
    )

    # ä¿å­˜
    save_menus(menus, args.output)


if __name__ == "__main__":
    main()
