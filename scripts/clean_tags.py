#!/usr/bin/env python3
"""
ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„:
- ç„¡æ„å‘³ãªã‚¿ã‚°ï¼ˆãŠã™ã™ã‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼‰ã®å‰Šé™¤
- å†—é•·ãªã‚¿ã‚°ï¼ˆä¾¡æ ¼å¸¯ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¿ã‚¤ãƒ—ï¼‰ã®å‰Šé™¤
- ã‚¿ã‚°åã®æ­£è¦åŒ–ï¼ˆæ‹¬å¼§ä»˜ãã‚¿ã‚°ã®çµ±ä¸€ãªã©ï¼‰
- é‡è¤‡ã‚¿ã‚°ã®å‰Šé™¤

ä½¿ç”¨æ–¹æ³•:
    python scripts/clean_tags.py [--dry-run]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    --dry-run  å®Ÿéš›ã®å¤‰æ›´ã‚’è¡Œã‚ãšã€å¤‰æ›´å†…å®¹ã®ã¿ã‚’è¡¨ç¤º
"""

import json
import shutil
from datetime import datetime
from pathlib import Path
from collections import Counter
import sys

# å‰Šé™¤å¯¾è±¡ã‚¿ã‚°ï¼ˆPhase 1 - æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ï¼‰
REMOVE_TAGS_PHASE1 = [
    # ç„¡æ„å‘³ãªã‚¿ã‚°
    "ãŠã™ã™ã‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼",  # ã»ã¼å…¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«ä»˜ä¸ã•ã‚Œã¦ãŠã‚Šç„¡æ„å‘³
    # ä¾¡æ ¼å¸¯ã‚¿ã‚°ï¼ˆpriceãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ¤œç´¢å¯èƒ½ï¼‰
    "ï½500å††",
    "500ï½1000å††",
    "1000ï½2000å††",
    "2000å††ï½",
    # ã‚µãƒ¼ãƒ“ã‚¹ã‚¿ã‚¤ãƒ—ã‚¿ã‚°ï¼ˆrestaurants.service_typesã«ç§»å‹•ã™ã¹ãï¼‰
    "ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹",
    "ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µãƒ¼ãƒ“ã‚¹",
    "ãƒ–ãƒƒãƒ•ã‚§ã‚µãƒ¼ãƒ“ã‚¹",
    "ãƒ¯ã‚´ãƒ³ã‚µãƒ¼ãƒ“ã‚¹",
    "ãƒãƒ•ã‚§ãƒ†ãƒªã‚¢ã‚µãƒ¼ãƒ“ã‚¹",
    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨é‡è¤‡
    "ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒƒã‚·ãƒ¥",
    "ã‚µã‚¤ãƒ‰",
]

# å‰Šé™¤å¯¾è±¡ã‚¿ã‚°ï¼ˆPhase 2 - å†—é•·ãƒ»é‡è¤‡ã‚¿ã‚°ï¼‰
REMOVE_TAGS_PHASE2 = [
    # ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«ãƒ‰ãƒªãƒ³ã‚¯é‡è¤‡
    "ãƒ‰ãƒªãƒ³ã‚¯ï¼ˆã‚¢ãƒ«ã‚³ãƒ¼ãƒ«ãƒ‰ãƒªãƒ³ã‚¯ï¼‰",  # 'ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«ãƒ‰ãƒªãƒ³ã‚¯'ã§ä»£æ›¿å¯èƒ½
    # ä¾¡æ ¼å¸¯ã‚¿ã‚°ï¼ˆæ®‹å­˜åˆ†ï¼‰
    "2000ï½4000å††",
    "4000å††ï½",
]

# çµ±åˆå‰Šé™¤ãƒªã‚¹ãƒˆ
REMOVE_TAGS = REMOVE_TAGS_PHASE1 + REMOVE_TAGS_PHASE2

# ã‚¿ã‚°æ­£è¦åŒ–ãƒãƒƒãƒ—
TAG_NORMALIZATION = {
    # æ‹¬å¼§ä»˜ãã‚¿ã‚°ã‚’çµ±ä¸€
    "ãƒ‰ãƒªãƒ³ã‚¯ï¼ˆã‚½ãƒ•ãƒˆãƒ‰ãƒªãƒ³ã‚¯ï¼‰": "ã‚½ãƒ•ãƒˆãƒ‰ãƒªãƒ³ã‚¯",
    "ã²ã‚“ã‚„ã‚Šï¼ˆã‚¢ã‚¤ã‚¹ï¼‰": "ã‚¢ã‚¤ã‚¹",
    "ã‚ã£ãŸã‹ã„ï¼ˆãƒ›ãƒƒãƒˆï¼‰": "ãƒ›ãƒƒãƒˆ",
    # è¡¨è¨˜ã‚†ã‚Œã®çµ±ä¸€
    "ãƒŸãƒƒã‚­ãƒ¼ãƒ¢ãƒãƒ¼ãƒ•ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼": "ãƒŸãƒƒã‚­ãƒ¼ãƒã‚¦ã‚¹",
}


def analyze_tags(menus: list) -> dict:
    """ã‚¿ã‚°ã®ä½¿ç”¨çŠ¶æ³ã‚’åˆ†æ"""
    all_tags = []
    menus_with_tags = 0
    tag_counter = Counter()

    for menu in menus:
        tags = menu.get("tags", [])
        if tags:
            menus_with_tags += 1
            all_tags.extend(tags)
            tag_counter.update(tags)

    return {
        "total_menus": len(menus),
        "menus_with_tags": menus_with_tags,
        "total_tags": len(all_tags),
        "unique_tags": len(tag_counter),
        "tag_counter": tag_counter,
    }


def clean_and_normalize_tags(data_path: Path, dry_run: bool = False) -> dict:
    """
    ã‚¿ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ­£è¦åŒ–ã‚’å®Ÿè¡Œ

    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        dry_run: True ã®å ´åˆã€å®Ÿéš›ã®å¤‰æ›´ã‚’è¡Œã‚ãªã„

    Returns:
        å®Ÿè¡Œçµæœã®çµ±è¨ˆæƒ…å ±
    """
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {data_path}")
    with open(data_path, "r", encoding="utf-8") as f:
        menus = json.load(f)

    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®åˆ†æ
    print("\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®åˆ†æ...")
    before_stats = analyze_tags(menus)
    print(f"   - ç·ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ•°: {before_stats['total_menus']:,} ä»¶")
    print(f"   - ã‚¿ã‚°ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ•°: {before_stats['menus_with_tags']:,} ä»¶")
    print(f"   - ç·ã‚¿ã‚°æ•°ï¼ˆé‡è¤‡å«ã‚€ï¼‰: {before_stats['total_tags']:,} å€‹")
    print(f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ã‚°æ•°: {before_stats['unique_tags']:,} å€‹")

    # å‰Šé™¤å¯¾è±¡ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’è¡¨ç¤º
    print("\nğŸ—‘ï¸  å‰Šé™¤å¯¾è±¡ã‚¿ã‚°ã®å‡ºç¾å›æ•°:")
    for tag in REMOVE_TAGS:
        count = before_stats["tag_counter"].get(tag, 0)
        if count > 0:
            print(f"   - {tag}: {count:,} å›")

    # æ­£è¦åŒ–å¯¾è±¡ã‚¿ã‚°ã®å‡ºç¾å›æ•°ã‚’è¡¨ç¤º
    print("\nğŸ”„ æ­£è¦åŒ–å¯¾è±¡ã‚¿ã‚°ã®å‡ºç¾å›æ•°:")
    for old_tag, new_tag in TAG_NORMALIZATION.items():
        count = before_stats["tag_counter"].get(old_tag, 0)
        if count > 0:
            print(f"   - {old_tag} â†’ {new_tag}: {count:,} å›")

    if dry_run:
        print("\nâš ï¸  --dry-run ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã€å®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã„ã¾ã›ã‚“")
        return before_stats

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_path = data_path.parent / f"menus_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    print(f"\nğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
    shutil.copy(data_path, backup_path)

    # å„ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã‚¿ã‚°ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    print("\nğŸ§¹ ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¸­...")
    changed_menus = 0
    removed_tag_count = 0
    normalized_tag_count = 0

    for menu in menus:
        original_tags = menu.get("tags", [])

        if not original_tags:
            continue

        # 1. å‰Šé™¤å¯¾è±¡ã‚¿ã‚°ã‚’é™¤å¤–
        cleaned_tags = [tag for tag in original_tags if tag not in REMOVE_TAGS]
        removed_tag_count += len(original_tags) - len(cleaned_tags)

        # 2. æ­£è¦åŒ–
        normalized_tags = []
        for tag in cleaned_tags:
            normalized_tag = TAG_NORMALIZATION.get(tag, tag)
            normalized_tags.append(normalized_tag)
            if tag != normalized_tag:
                normalized_tag_count += 1

        # 3. é‡è¤‡å‰Šé™¤ï¼ˆé †åºã‚’ä¿æŒï¼‰
        unique_tags = list(dict.fromkeys(normalized_tags))

        # ã‚¿ã‚°ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
        if set(original_tags) != set(unique_tags):
            changed_menus += 1

        menu["tags"] = unique_tags

    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    print(f"\nğŸ’¾ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {data_path}")
    with open(data_path, "w", encoding="utf-8") as f:
        json.dump(menus, f, ensure_ascii=False, indent=2)

    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®åˆ†æ
    print("\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®åˆ†æ...")
    after_stats = analyze_tags(menus)
    print(f"   - ç·ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ•°: {after_stats['total_menus']:,} ä»¶")
    print(f"   - ã‚¿ã‚°ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ•°: {after_stats['menus_with_tags']:,} ä»¶")
    print(f"   - ç·ã‚¿ã‚°æ•°ï¼ˆé‡è¤‡å«ã‚€ï¼‰: {after_stats['total_tags']:,} å€‹")
    print(f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ã‚°æ•°: {after_stats['unique_tags']:,} å€‹")

    # ã‚µãƒãƒªãƒ¼
    print("\nâœ… ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    print(f"   - å¤‰æ›´ã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼æ•°: {changed_menus:,} ä»¶")
    print(f"   - å‰Šé™¤ã•ã‚ŒãŸã‚¿ã‚°æ•°: {removed_tag_count:,} å€‹")
    print(f"   - æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚°æ•°: {normalized_tag_count:,} å€‹")
    print(
        f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ã‚°æ•°ã®å¤‰åŒ–: {before_stats['unique_tags']} â†’ {after_stats['unique_tags']} ({after_stats['unique_tags'] - before_stats['unique_tags']:+d})"
    )

    # TOP 30 ã‚¿ã‚°ã‚’è¡¨ç¤º
    print("\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã® TOP 30 ã‚¿ã‚°:")
    for i, (tag, count) in enumerate(after_stats["tag_counter"].most_common(30), 1):
        print(f"   {i:2d}. {tag}: {count:,} å›")

    return {
        "before": before_stats,
        "after": after_stats,
        "changed_menus": changed_menus,
        "removed_tag_count": removed_tag_count,
        "normalized_tag_count": normalized_tag_count,
    }


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    dry_run = "--dry-run" in sys.argv

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    data_path = Path(__file__).parent.parent / "data" / "menus.json"

    if not data_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        sys.exit(1)

    print("=" * 80)
    print("ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 80)
    print()

    try:
        result = clean_and_normalize_tags(data_path, dry_run=dry_run)

        if not dry_run:
            print("\n" + "=" * 80)
            print("âœ… å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            print("=" * 80)
            print()
            print("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¾©å…ƒã™ã‚‹å ´åˆ:")
            print("  cp data/menus_backup_YYYYMMDD_HHMMSS.json data/menus.json")
            print()

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
